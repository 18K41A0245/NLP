{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPz3NrezMymWcrQbQCM8Ihb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/18K41A0245/NLP/blob/main/Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiD6pNPATYYK",
        "outputId": "8a038868-6308-48cd-ad6d-83084251850d"
      },
      "source": [
        "#1.Split the above paragraph into sentences\n",
        "\n",
        "d=\"Are you fascinated by the amount of text data available on the internet? Are you looking for ways to work with this text data but aren’t sure where to begin? Machines, after all, recognize numbers, not the letters of our language. And that can be a tricky landscape to navigate in machine learning.\"\n",
        "sentence = []\n",
        "r = d.split('?')\n",
        "for i in r:\n",
        "  sentence.extend(i.split(\".\"))\n",
        "print(sentence)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Are you fascinated by the amount of text data available on the internet', ' Are you looking for ways to work with this text data but aren’t sure where to begin', ' Machines, after all, recognize numbers, not the letters of our language', ' And that can be a tricky landscape to navigate in machine learning', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGRLdtvCVL35",
        "outputId": "59872a5c-d42e-4e94-a236-72b5f2c356f9"
      },
      "source": [
        "#2.Split the above paragraph into words\n",
        "\n",
        "word = d.split( )\n",
        "print(word)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Are', 'you', 'fascinated', 'by', 'the', 'amount', 'of', 'text', 'data', 'available', 'on', 'the', 'internet?', 'Are', 'you', 'looking', 'for', 'ways', 'to', 'work', 'with', 'this', 'text', 'data', 'but', 'aren’t', 'sure', 'where', 'to', 'begin?', 'Machines,', 'after', 'all,', 'recognize', 'numbers,', 'not', 'the', 'letters', 'of', 'our', 'language.', 'And', 'that', 'can', 'be', 'a', 'tricky', 'landscape', 'to', 'navigate', 'in', 'machine', 'learning.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3tmMt8FdyRo",
        "outputId": "4902a061-285e-4f03-d35c-1046c6abd582"
      },
      "source": [
        "#3. Find stem and lemma words for the given words?\n",
        "\n",
        "from nltk import PorterStemmer\n",
        "from nltk import WordNetLemmatizer\n",
        "data = [\"cats\",\"trouble\",\"troubling\",\"troubled\",\"having\",\"Corriendo\",\"at\",\"was\"]\n",
        "ps = PorterStemmer()\n",
        "print(\"Stem of given words is: \")\n",
        "for i in data:\n",
        "  print(\"stem of\",i,\"is:\", ps.stem(i))\n",
        "l = WordNetLemmatizer()\n",
        "print(\"Lemma of given words is: \")\n",
        "for word in data:\n",
        "  print(\"Lemma of\",word,\"is:\", l.lemmatize(word))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stem of given words is: \n",
            "stem of cats is: cat\n",
            "stem of trouble is: troubl\n",
            "stem of troubling is: troubl\n",
            "stem of troubled is: troubl\n",
            "stem of having is: have\n",
            "stem of Corriendo is: corriendo\n",
            "stem of at is: at\n",
            "stem of was is: wa\n",
            "Lemma of given words is: \n",
            "Lemma of cats is: cat\n",
            "Lemma of trouble is: trouble\n",
            "Lemma of troubling is: troubling\n",
            "Lemma of troubled is: troubled\n",
            "Lemma of having is: having\n",
            "Lemma of Corriendo is: Corriendo\n",
            "Lemma of at is: at\n",
            "Lemma of was is: wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vl1BCq9e7qq",
        "outputId": "3dfa0506-a987-4527-dca6-373bb1ad0df6"
      },
      "source": [
        "#4. Find stop words from the given paragraph?\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"The NLTK library  is  one  of  the  oldest  and  most  commonly  used  Python  libraries  for  Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the  corpus  module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.\"\n",
        "text_tokens = word_tokenize(text)\n",
        "text_tokens = [word.lower() for word in text_tokens]\n",
        "stop_words = [word for word in text_tokens if word in stopwords.words(\"english\")]\n",
        "print(stop_words)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['the', 'is', 'of', 'the', 'and', 'most', 'for', 'and', 'you', 'can', 'the', 'of', 'in', 'the', 'to', 'from', 'a', 'you', 'can', 'your', 'into', 'and', 'then', 'the', 'if', 'it', 'in', 'the', 'of', 'by']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mBqG2MumEiH",
        "outputId": "6aa04e67-d868-4b36-e697-38ddd0a97d94"
      },
      "source": [
        "#5. From the above paragraph print frequency of each word using NLTK?\n",
        "\n",
        "freq = {}\n",
        "for word in text_tokens:\n",
        "  freq[word] = freq.get(word, 0) + 1\n",
        "print(freq)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 6, 'nltk': 3, 'library': 1, 'is': 1, 'one': 1, 'of': 3, 'oldest': 1, 'and': 3, 'most': 1, 'commonly': 1, 'used': 1, 'python': 1, 'libraries': 1, 'for': 1, 'natural': 1, 'language': 1, 'processing': 1, '.': 3, 'supports': 1, 'stop': 4, 'word': 2, 'removal': 1, ',': 2, 'you': 2, 'can': 2, 'find': 1, 'list': 2, 'words': 4, 'in': 2, 'corpus': 1, 'module': 1, 'to': 1, 'remove': 2, 'from': 1, 'a': 1, 'sentence': 1, 'divide': 1, 'your': 1, 'text': 1, 'into': 1, 'then': 1, 'if': 1, 'it': 1, 'exits': 1, 'provided': 1, 'by': 1}\n"
          ]
        }
      ]
    }
  ]
}